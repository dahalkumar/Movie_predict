{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.movie_predictor.constants import *\n",
    "import os\n",
    "from box.exceptions import BoxValueError\n",
    "import yaml\n",
    "from src.movie_predictor import logger\n",
    "import json\n",
    "import joblib\n",
    "from ensure import ensure_annotations\n",
    "from box import ConfigBox\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_yaml(path_to_yaml: Path) -> ConfigBox:\n",
    "    \"\"\"reads yaml file and returns\n",
    "    Args:\n",
    "        path_to_yaml (str): input is path\n",
    "    Raises:\n",
    "        ValueError: if yaml file is empty\n",
    "        e: empty file\n",
    "    Returns:\n",
    "        ConfigBox: ConfigBox type\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path_to_yaml) as yaml_file:\n",
    "            config_yaml = yaml.safe_load(yaml_file)\n",
    "            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\n",
    "            return ConfigBox(config_yaml)\n",
    "    except BoxValueError:\n",
    "        raise ValueError(\"yaml file is empty\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "@ensure_annotations\n",
    "def create_directories(path_to_directories: list, verbose=True):\n",
    "    \"\"\"create list of directories\n",
    "    Args:\n",
    "        path_to_directories (list): list of path of directories\n",
    "        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\n",
    "    \"\"\"\n",
    "    for path in path_to_directories:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        if verbose:\n",
    "            logger.info(f\"created directory at: {path}\")\n",
    "\n",
    "@ensure_annotations\n",
    "def save_json(path: Path, data: dict):\n",
    "    \"\"\"save json data\n",
    "    Args:\n",
    "        path (Path): path to json file\n",
    "        data (dict): data to be saved in json file\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    logger.info(f\"json file saved at: {path}\")\n",
    "\n",
    "@ensure_annotations\n",
    "def load_json(path: Path) -> ConfigBox:\n",
    "    \"\"\"load json files data\n",
    "    Args:\n",
    "        path (Path): path to json file\n",
    "    Returns:\n",
    "        ConfigBox: data as class attributes instead of dict\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        config_yaml = json.load(f)\n",
    "\n",
    "    logger.info(f\"json file loaded succesfully from: {path}\")\n",
    "    return ConfigBox(config_yaml)\n",
    "\n",
    "@ensure_annotations\n",
    "def save_bin(data: Any, path: Path):\n",
    "    \"\"\"save binary file\n",
    "    Args:\n",
    "        data (Any): data to be saved as binary\n",
    "        path (Path): path to binary file\n",
    "    \"\"\"\n",
    "    joblib.dump(value=data, filename=path)\n",
    "    logger.info(f\"binary file saved at: {path}\")\n",
    "\n",
    "@ensure_annotations\n",
    "def load_bin(path: Path) -> Any:\n",
    "    \"\"\"load binary data\n",
    "    Args:\n",
    "        path (Path): path to binary file\n",
    "    Returns:\n",
    "        Any: object stored in the file\n",
    "    \"\"\"\n",
    "    data = joblib.load(path)\n",
    "    logger.info(f\"binary file loaded from: {path}\")\n",
    "    return data\n",
    "\n",
    "@ensure_annotations\n",
    "def load_data(file_path: str, schema_file_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        datatset_schema = read_yaml(schema_file_path)\n",
    "\n",
    "        schema = datatset_schema[DATASET_SCHEMA_COLUMNS_KEY]\n",
    "\n",
    "        dataframe = pd.read_csv(file_path)\n",
    "\n",
    "        error_messgae = \"\"\n",
    "\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            if column in list(schema.keys()):\n",
    "                dataframe[column].astype(schema[column])\n",
    "            else:\n",
    "                error_messgae = f\"{error_messgae} \\nColumn: [{column}] is not in the schema.\"\n",
    "        if len(error_messgae) > 0:\n",
    "            raise Exception(error_messgae)\n",
    "        return dataframe\n",
    "\n",
    "    except Exception as e:\n",
    "        return e\n",
    "@ensure_annotations    \n",
    "def get_size(path: Path) -> str:\n",
    "    \"\"\"get size in KB\n",
    "    Args:\n",
    "        path (Path): path of the file\n",
    "    Returns:\n",
    "        str: size in KB\n",
    "    \"\"\"\n",
    "    size_in_kb = round(os.path.getsize(path)/1024)\n",
    "    return f\"~ {size_in_kb} KB\"\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def save_numpy_array_data(file_path: Path, array: np.array):\n",
    "    \"\"\"\n",
    "    Save numpy array data to file\n",
    "    file_path: str location of file to save\n",
    "    array: np.array data to save\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, 'wb') as file_obj:\n",
    "            np.save(file_obj, array)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "    \n",
    "@ensure_annotations\n",
    "def save_object(file_path:str,obj):\n",
    "    \"\"\"\n",
    "    file_path: str\n",
    "    obj: Any sort of object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            joblib.dump(obj, file_obj)\n",
    "    except Exception as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformConfig:\n",
    "    root_dir: Path\n",
    "    tranfored_train_dir: Path\n",
    "    transormed_test_dir: Path\n",
    "    preprocessed_file_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BUDGET_XLSX = 'budget.xlsx'\n",
    "BUDGET_FILE_PATH = os.path.join(CONFIG_DIR,BUDGET_XLSX)\n",
    "\n",
    "COMBINE_XLSX = 'combine.xlsx'\n",
    "COMBINE_FILE_PATH = os.path.join(CONFIG_DIR,COMBINE_XLSX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = pd.read_excel(BUDGET_FILE_PATH)\n",
    "df1 = pd.read_excel(COMBINE_FILE_PATH,sheet_name='runtime')\n",
    "df2 = pd.read_excel(COMBINE_FILE_PATH,sheet_name='opening_theaters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml(CONFIG_FILE_PATH)\n",
    "schema = read_yaml(DATA_VALIDATION_FILE)\n",
    "create_directories([config.artifacts_root])\n",
    "\n",
    "transform_config = config.data_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifacts/data_transformation'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_config.root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(DATA_VALIDATION_FILE)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_transform_config(self) -> DataTransformConfig:\n",
    "        transform_config = self.config.data_transformation\n",
    "\n",
    "        create_directories([transform_config.root_dir])\n",
    "        create_directories([transform_config.transformed_train_dir])\n",
    "        create_directories([transform_config.transformed_test_dir])\n",
    "        create_directories([transform_config.preprocessed_dir])\n",
    "\n",
    "        data_transform_config = DataTransformConfig(\n",
    "            root_dir = transform_config.root_dir,\n",
    "            tranfored_train_dir = transform_config.transformed_train_dir,\n",
    "            transormed_test_dir = transform_config.transformed_test_dir,\n",
    "            preprocessed_file_path = transform_config.preprocessed_object_file_name\n",
    "        )\n",
    "        return data_transform_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformConfig(root_dir='artifacts/data_transformation', tranfored_train_dir='artifacts/data_transformation/train', transormed_test_dir='artifacts/data_transformation/test', preprocessed_file_path='artifacts/data_transformation/preprocessed/preprocessed.pkl')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConfigurationManager().get_data_transform_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "DataIngestionArtifact = namedtuple(\"DataIngestionArtifact\",\n",
    "[ \"train_file_path\", \"test_file_path\", \"is_ingested\", \"message\"])\n",
    "\n",
    "DataValidationArtifact = namedtuple(\"DataValidationArtifact\",\n",
    "[ \"report_file_path\",\"report_page_file_path\",\"is_validated\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SCHEMA_COLUMNS_KEY = \"columns\"\n",
    "NUMERICAL_COLUMN_KEY = \"numerical_columns\"\n",
    "CATEGORICAL_COLUMN_KEY = \"categorical_columns\"\n",
    "TARGET_COLUMN_KEY = \"target_column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatset_schema = read_yaml(DATA_VALIDATION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "class ReplaceCharsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X[self.columns] = X[self.columns].applymap(self._replace_chars)\n",
    "        for col in self.columns:\n",
    "            if X[col].dtype == 'O' or X[col].dtype == 'float':\n",
    "                X[col].replace('\\\\N', np.nan, inplace=True)\n",
    "                \n",
    "            if col in ['budget','opening_theaters','world_revenue','runtimeMinutes']:\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')    \n",
    "    \n",
    "        return X\n",
    "    \n",
    "    def _replace_chars(self, cell):\n",
    "        if isinstance(cell, str):\n",
    "            cell_str = str(cell)\n",
    "            cell_str = cell_str.replace(',', '').replace('$', '')\n",
    "            return cell_str\n",
    "        else:\n",
    "            return cell\n",
    "\n",
    "\n",
    "\n",
    "class DropDuplicatesTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.drop_duplicates()\n",
    "    \n",
    "class FillMissingBudgets(TransformerMixin):\n",
    "    def __init__(self, df3):\n",
    "        self.d_p = dict(zip(df3['originalTitle'], df3['budget']))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['budget'] = X['budget'].fillna(X['originalTitle'].map(self.d_p))\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class FillnaTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column, dictionary):\n",
    "        self.column = column\n",
    "        self.dictionary = dictionary\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X[self.column] = X[self.column].fillna(X['tconst'].map(self.dictionary))\n",
    "        return X    \n",
    "\n",
    "\n",
    "class DropColumnsTransformer(TransformerMixin):\n",
    "    \"\"\"Custom transformer to drop columns from a Pandas DataFrame.\"\"\"\n",
    "    \n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X.drop(columns=self.columns_to_drop)\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.labels_ordered_ = {}\n",
    "        categorical_features = [feature for feature in X.columns if X[feature].dtype == 'O']\n",
    "        for feature in categorical_features:\n",
    "            labels_ordered = X.groupby([feature]).size().sort_values().index\n",
    "            labels_ordered = {value: index for index, value in enumerate(labels_ordered, 0)}\n",
    "            self.labels_ordered_[feature] = labels_ordered\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        for feature, labels_ordered in self.labels_ordered_.items():\n",
    "            X[feature] = X[feature].map(labels_ordered)\n",
    "        return X.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.movie_predictor.entity.artifact_entity import DataTransformationArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "class DataTransform:\n",
    "    def __init__(self, data_transformation: DataTransformConfig,\n",
    "                 data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 data_validation_artifact: DataValidationArtifact) -> None:\n",
    "        try:\n",
    "            \n",
    "            self.data_transformation_config = data_transformation\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            return e\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_data_transformer_object(self,dataframe) -> ColumnTransformer:\n",
    "            try:\n",
    "                # train_file_path = self.data_ingestion_artifact.train_file_path\n",
    "                # test_file_path = self.data_ingestion_artifact.test_file_path\n",
    "                \n",
    "                datatset_schema = read_yaml(DATA_VALIDATION_FILE)\n",
    "                df3 = pd.read_excel('budget.xlsx')\n",
    "                df1 = pd.read_excel('combine.xlsx',sheet_name='runtime')\n",
    "                df2 = pd.read_excel('combine.xlsx',sheet_name='opening_theaters')\n",
    "                d = dict(zip(df2['tconst'], df2['opening_theaters']))\n",
    "                p = dict(zip(df1['tconst'], df1['runtimeMinutes']))\n",
    "\n",
    "                # Define a custom transformer class to fill null values based on a dictionary\n",
    "                # Create a pipeline that applies the FillnaTransformer to fill null values in two columns\n",
    "                # Create a pipeline that applies the ReplaceCharsTransformer to the DataFrame for specific columns\n",
    "                pipeline_duplicate = Pipeline([\n",
    "                    ('remove_duplicates', DropDuplicatesTransformer())\n",
    "                ])\n",
    "\n",
    "                # Apply the pipeline to the dataframe\n",
    "\n",
    "                conversion_pipeline = Pipeline([\n",
    "                                    ('replace_chars', ReplaceCharsTransformer(columns=['budget','opening_theaters','world_revenue','runtimeMinutes','genres_y','job','characters','birthYear','deathYear','knownForTitles'])),\n",
    "                                    ('passthrough', 'passthrough')\n",
    "                ])\n",
    "                mpaa_missing_pipeline = Pipeline([\n",
    "                                    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "                ])\n",
    "                                \n",
    "                manual_nan_pipeline = Pipeline([\n",
    "                                    ('fillna_runtime', FillnaTransformer(column='runtimeMinutes', dictionary=p)),\n",
    "                                    ('fillna_opening', FillnaTransformer(column='opening_theaters', dictionary=d)),\n",
    "                                    ('fill_budgets', FillMissingBudgets(df3))\n",
    "                ])\n",
    "                cols_drop = ['Unnamed: 0','genres_y','domestic_revenue','opening_revenue','nconst','deathYear','job','characters','birthYear','primaryProfession','knownForTitles','isAdult','titleType','tconst']\n",
    "              \n",
    "\n",
    "                drop_missing_pipeline = Pipeline([\n",
    "                    ('drop_cols', DropColumnsTransformer(columns_to_drop=cols_drop))\n",
    "                ])\n",
    "\n",
    "                encoding_scaling_pipeline = Pipeline([\n",
    "                    ('encoder', CategoricalEncoder()),\n",
    "                     ('scaler', MinMaxScaler())\n",
    "                ])\n",
    "                preprocessing = ColumnTransformer([\n",
    "                ('pipeline_duplicate', pipeline_duplicate, dataframe),\n",
    "                ('conversion_pipeline', conversion_pipeline, dataframe),\n",
    "                ('mpaa_missing_pipeline', mpaa_missing_pipeline, dataframe),\n",
    "                ('manual_nan_pipeline', manual_nan_pipeline, dataframe),\n",
    "                ('drop_missing_pipeline', drop_missing_pipeline, dataframe),\n",
    "                ('encoding_scaling_pipeline', encoding_scaling_pipeline, dataframe)\n",
    "\n",
    "                ])\n",
    "                return preprocessing\n",
    "\n",
    "                \n",
    "                # df_clean = pipeline_duplicate.fit_transform(dataframe)\n",
    "                # df_clean = conversion_pipeline.fit_transform(df_clean)\n",
    "                # df_clean['MPAA'] = mpaa_missing_pipeline.fit_transform(df_clean[['MPAA']])\n",
    "\n",
    "                # new_df = manual_nan_pipeline.fit_transform(df_clean)\n",
    "                # transformed_df = drop_missing_pipeline.fit_transform(new_df)\n",
    "                # coded = encoding_scaling_pipeline.fit_transform(transformed_df)\n",
    "            \n",
    "            except Exception as e:\n",
    "                return e    \n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    def run_data_transformation(self)->DataTransformationArtifact:\n",
    "        try:\n",
    "            \n",
    "            \n",
    "\n",
    "            #logging.info(f\"Obtaining training and test file path.\")\n",
    "            train_file_path = self.data_ingestion_artifact.train_file_path\n",
    "            test_file_path = self.data_ingestion_artifact.test_file_path\n",
    "            \n",
    "\n",
    "            #schema_file_path = self.data_validation_artifact.schema_file_path\n",
    "            datatset_schema = read_yaml(DATA_VALIDATION_FILE)\n",
    "            # logging.info(f\"Loading training and test data as pandas dataframe.\")\n",
    "            train_df = load_data(file_path=train_file_path, schema_file_path=datatset_schema)\n",
    "            \n",
    "            test_df = load_data(file_path=test_file_path, schema_file_path=datatset_schema)\n",
    "\n",
    "            schema = read_yaml(file_path=datatset_schema)\n",
    "\n",
    "            target_column_name = schema[TARGET_COLUMN_KEY]\n",
    "\n",
    "\n",
    "            #logging.info(f\"Splitting input and target feature from training and testing dataframe.\")\n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name],axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name],axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "            \n",
    "\n",
    "            #logging.info(f\"Applying preprocessing object on training dataframe and testing dataframe\")\n",
    "            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "\n",
    "            train_arr = np.c_[ input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "            # root_dir: Path\n",
    "            # tranfored_train_dir: Path\n",
    "            # transormed_test_dir: Path\n",
    "            # preprocessed_file_path: Path\n",
    "            transformed_train_dir = self.data_transformation_config.tranfored_train_dir\n",
    "            transformed_test_dir = self.data_transformation_config.transormed_test_dir\n",
    "            #since we have numpy array data we replace file extension in npz\n",
    "            train_file_name = os.path.basename(train_file_path).replace(\".csv\",\".npz\")\n",
    "            test_file_name = os.path.basename(test_file_path).replace(\".csv\",\".npz\")\n",
    "\n",
    "            transformed_train_file_path = os.path.join(transformed_train_dir, train_file_name)\n",
    "            transformed_test_file_path = os.path.join(transformed_test_dir, test_file_name)\n",
    "\n",
    "            #logging.info(f\"Saving transformed training and testing array.\")\n",
    "            \n",
    "            save_numpy_array_data(file_path=transformed_train_file_path,array=train_arr)\n",
    "            save_numpy_array_data(file_path=transformed_test_file_path,array=test_arr)\n",
    "\n",
    "            \n",
    "            preprocessing_obj_file_path = self.data_transformation_config.preprocessed_file_path\n",
    "            \n",
    "            preprocessing_obj = self.get_data_transformer_object(datatset_schema)\n",
    "            \n",
    "            # logging.info(f\"Saving preprocessing object.\")\n",
    "            save_object(file_path=preprocessing_obj_file_path, obj=preprocessing_obj)\n",
    "\n",
    "            data_transformation_artifact = DataTransformationArtifact(is_transformed=True,\n",
    "            message=\"Data transformation successfull.\",\n",
    "            transformed_train_file_path=transformed_train_file_path,\n",
    "            transformed_test_file_path=transformed_test_file_path,\n",
    "            preprocessed_object_file_path=preprocessing_obj_file_path\n",
    "\n",
    "            )\n",
    "            # logging.info(f\"Data transformationa artifact: {data_transformation_artifact}\")\n",
    "            return data_transformation_artifact\n",
    "        except Exception as e:\n",
    "            return e        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = ConfigurationManager()\n",
    "transform_config = schema.get_data_transform_config()\n",
    "data_transform = DataTransform(data_transformation=transform_config,\n",
    "                                    data_ingestion_artifact=DataIngestionArtifact,\n",
    "                                    data_validation_artifact=DataValidationArtifact)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ensure.main.EnsureError(\"Argument file_path of type <class '_collections._tuplegetter'> to <function load_data at 0x7f65b90be950> does not match annotation type <class 'str'>\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transform.run_data_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataTransform at 0x7f65b8efb280>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ingestion_artifact =  DataIngestionArtifact       \n",
    "data_transformation_config = DataTransformConfig\n",
    "train_file_path = data_ingestion_artifact.train_file_path\n",
    "test_file_path = data_ingestion_artifact.test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_tuplegetter(0, 'Alias for field number 0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.movie_predictor.config.configuration import Configuartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configuartion.training_pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.movie_predictor.config.configuration.Configuartion"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df8635527aba43c0978661b8f3d9ea7dfe51393f6f96b3dcf9d26e89056f8639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
